{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No Limit Texas Hold'em Deep Q learning\n",
    "\n",
    "This notebook is using modern deep learning libraries to try to solve No Limit Hold'em (NLH). There are AIs developed that have beaten world class players in heads-up (2 players) NLH. We still have a long way to go here.\n",
    "\n",
    "To run the notebook you need to install the https://github.com/VinQbator/holdem fork of the holdem library. A lot of bugfixes and changes were needed to effectively run the environment for deep learning.\n",
    "\n",
    "Also keras-rl should be installed from https://github.com/VinQbator/keras-rl. Sorry for the inconvenience.\n",
    "\n",
    "Rest of the libraries are found from pip as listed in the following imports section.\n",
    "\n",
    "A lot of heavy lifting is in .py files adjacent to the notebook to keep the notebook clean.\n",
    "\n",
    "Most of the effort here is put into building a framework to enable more serious development in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from players.atm import ATM\n",
    "from players.ai_player import AIPlayer\n",
    "from players.random_player import RandomPlayer\n",
    "from training_env import TrainingEnv\n",
    "from agents import build_dqn_agent, fit_agent, train_loop, load_agent_weights\n",
    "from models import simple_model, complex_model, test_model\n",
    "from util import visualize_history, use_jupyter, set_on_demand_memory_allocation\n",
    "from helpers.poker_history import PokerHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_on_demand_memory_allocation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_jupyter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "### Some of the stuff that is happening in the wrapper layer for the holdem gym environment.\n",
    "* Positions are one-hot encoded\n",
    "* Pot and bet sizes are normalized to 100 big blinds\n",
    "* Hand ranking either normalized or one-hot encoded\n",
    "* Cards are one-hot encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Action space\n",
    "\n",
    "### Essentially infinite or at least large action space is split up into\n",
    "\n",
    "* Base moves like FOLD/CALL/CHECK\n",
    "\n",
    "* And few common bet/raise sizes relative to the pot size: 1/5, 1/4, 1/3, 2/5, 1/2, 3/5, 2/3, 3/4, 4/5, 1, 4/3, 5/3, 2, 3, 5, 10, 15, 20, 30, 50, 75, 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many players in table\n",
    "NUMBER_OF_SEATS = 2\n",
    "# Max betsize in simulation environment (shouldn't really matter with discrete relative to pot sizing)\n",
    "MAX_BET = 100000\n",
    "# 'norm' (normalized) or 'one-hot' < how to encode player hand ranking from 7642 unique values\n",
    "RANK_ENCODING = 'norm'\n",
    "\n",
    "WINDOW = 1\n",
    "MODEL = simple_model\n",
    "\n",
    "FIRST_RUN_STEPS = 200\n",
    "SECOND_RUN_STEPS = 200\n",
    "THIRD_RUN_STEPS = 200\n",
    "THIRD_RUN_ITERATIONS = 10\n",
    "\n",
    "BENCHMARK_EPISODES = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets start with playing against player that always calls or checks based on which is currently valid move\n",
    "# Hopefully this will teach the agent something about hand strength at least\n",
    "env = TrainingEnv.build_environment(ATM(), NUMBER_OF_SEATS, debug=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### First lets train a simple model with 1 step sequences against an opponent who always calls or folds based on whichever move is valid\n",
    "\n",
    "No need to train against it for long - we just want to learn some basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MODEL(WINDOW, env.n_observation_dimensions, env.n_actions)\n",
    "print(model.summary())\n",
    "# window_length - how many timesteps to look into past (will multiply observation space by this, be careful)\n",
    "# enable_double_dqn - https://arxiv.org/pdf/1509.06461.pdf\n",
    "# enable_dueling_network - ???\n",
    "# train_interval - every how many steps to run a train cycle (or if in 0...1 range, the soft update weight)\n",
    "# n_warmup_steps - how many steps to run without training\n",
    "# batch_size - number of (s, a, G) triplets to train on in one training cycle (as a batch)\n",
    "# gamma - future reward discount essentially\n",
    "# memory_interval - how often to add last step to memory buffer (discarding every other)\n",
    "\n",
    "agent = build_dqn_agent(model, env.n_actions, window_length=WINDOW, target_model_update=0.001, \n",
    "                        enable_double_dqn=True, enable_dueling_network=True, dueling_type='avg', \n",
    "                        train_interval=100, n_warmup_steps=50, batch_size=32, gamma=.99, memory_interval=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Let's play for 100000 steps (decisions made by AI)\n",
    "agent, hist = fit_agent(agent, env, FIRST_RUN_STEPS, debug=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## As we can see the simple network was able to learn a bit and achieve a positive winrate\n",
    "\n",
    "NB! The Winrate and Winnings plots show $ not big blinds. (Didn't want to run the notebook again, fixed in code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some plots of how the training session went\n",
    "visualize_history(hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hand history rendering is still a bit wonky, but it's clear that the bot is not making too smart choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's evaluate our agent for 5 episodes (hands).\n",
    "agent.test(env, nb_episodes=5, visualize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets now play against opponent who makes totally random moves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets play against our bot with totally random moves now\n",
    "# Hopefully it teaches the agent at least something about how to act on wide range of situations\n",
    "env = TrainingEnv.build_environment(RandomPlayer(), NUMBER_OF_SEATS, debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Train for playing against RandomPlayer\n",
    "agent, hist = fit_agent(agent, env, SECOND_RUN_STEPS, False, hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our winrate has increased even if the opponent is not totally predictable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_history(hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hand history shows that AI is still making quite random moves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "agent.test(env, nb_episodes=5, visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "agent, hist = train_loop(agent, MODEL, env, steps_in_iteration=THIRD_RUN_STEPS, \n",
    "                         n_iterations=THIRD_RUN_ITERATIONS, window_length=WINDOW, verbose=1, debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.test(env, nb_episodes=50, visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_agent_weights(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's benchmark against ATM\n",
    "env = TrainingEnv.build_environment(ATM(), n_seats=NUMBER_OF_SEATS)\n",
    "hist = agent.test(env, nb_episodes=BENCHMARK_EPISODES, visualize=False, verbose=0, history=PokerHistory())\n",
    "visualize_history(hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's benchmark against RandomPlayer\n",
    "env = TrainingEnv.build_environment(RandomPlayer(), n_seats=NUMBER_OF_SEATS)\n",
    "hist = agent.test(env, nb_episodes=BENCHMARK_EPISODES, visualize=False, verbose=0, history=PokerHistory())\n",
    "visualize_history(hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
