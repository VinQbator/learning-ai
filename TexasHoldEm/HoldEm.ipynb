{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import holdem\n",
    "from holdem.utils import action_table, player_table, community_table\n",
    "from treys import Card, Deck, Evaluator\n",
    "from players.atm import ATM\n",
    "from players.ai_player import AIPlayer\n",
    "from players.random_player import RandomPlayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Dense, Activation, Flatten, Dropout, Input, Concatenate\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.agents.ddpg import DDPGAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.core import Env\n",
    "from rl.random import OrnsteinUhlenbeckProcess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training_env import TrainingEnv\n",
    "from helpers.poker_history import PokerHistory\n",
    "from util import visualize_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doing now the essential changes\n",
    "* Ignore Empty Seats (not at the moment)\n",
    "* Position to one-hot encoded\n",
    "* Stack will be normalized to 100 big blinds\n",
    "* Hand ranking add one-hot encoded\n",
    "* Sidepot normalized to 100 big blinds\n",
    "* Cards to one-hot encoding\n",
    "* Ignore big blind size (using normalization anyways) (not at the moment)\n",
    "\n",
    "### Ideas to do later\n",
    "* Pot odds\n",
    "* Flush draw outs\n",
    "* Flush draw strength (Nuts, 2nd nuts etc.)\n",
    "* Straight draw outs\n",
    "* Straight draw strength\n",
    "* Paired pocket cards\n",
    "* Suited pocket cards\n",
    "* Opponent stats\n",
    "* Convolution over card matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-Hot Ranking\n",
    "#### n_seats times the whole block for each opponent\n",
    "\n",
    "* 0 dimensions for seat occupied\n",
    "* n_seats dimensions for one-hot encoded position\n",
    "* 1 dimension for normalized stack\n",
    "* 1 dimensions for is player playing\n",
    "* 7642 + 1 (missing) dimensions for one-hot encoded hand ranking or 1 if normalized\n",
    "* 1 dimension for playing round\n",
    "* 1 dimension for betting\n",
    "* 1 dimension for all-in\n",
    "* 1 dimension for last sidepot\n",
    "\n",
    "###### SubTotal = (7463 or 1) + (6 + n_seats) * n_seats\n",
    "\n",
    "* 52 dimensions for each one-hot encoded pocket card (104 total)\n",
    "\n",
    "* 53 (can be blank) dimensions for each one-hot encoded community card (265 total)\n",
    "\n",
    "* n_seats dimensions for dealer button position\n",
    "* 1 dimension for normalized small blind size\n",
    "* 1 dimension for big blind size\n",
    "* 1 dimension for normalized pot size\n",
    "* 1 dimension for normalized last raise\n",
    "* 1 dimension for normalized min raise size\n",
    "* 1 dimension for normalized amount to call\n",
    "* n_seats dimensions for current player position\n",
    "\n",
    "###### SubTotal = 375 + 2 * n_seats\n",
    "### Total = 375 + 2 * n_seats + (7463 or 1) + (6 + n_seats) * n_seats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many players in table\n",
    "NUMBER_OF_SEATS = 2\n",
    "# Max betsize in simulation environment (shouldn't really matter with discrete relative to pot sizing)\n",
    "MAX_BET = 100000\n",
    "# 'norm' (normalized) or 'one-hot' < how to encode player hand ranking from 7642 unique values\n",
    "RANK_ENCODING = 'one-hot'\n",
    "\n",
    "DEBUG = False\n",
    "\n",
    "FIRST_RUN_STEPS = 100000\n",
    "SECOND_RUN_STEPS = 100000\n",
    "THIRD_RUN_STEPS = 500000\n",
    "THIRD_RUN_ITERATIONS = 3\n",
    "THIRD_RUN_WINDOW = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress FutureWarnings that trash the output\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_environment(opponent, debug):\n",
    "    env = gym.make('TexasHoldem-v1', n_seats=NUMBER_OF_SEATS, max_limit=MAX_BET)\n",
    "    other_players = [opponent for i in range(NUMBER_OF_SEATS - 1)]\n",
    "    return TrainingEnv(env, other_players, NUMBER_OF_SEATS, debug=debug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_agent(model, n_actions, window_length, debug):\n",
    "    memory = SequentialMemory(limit=20000//window_length, window_length=window_length)\n",
    "    policy = BoltzmannQPolicy()\n",
    "    agent = DQNAgent(model=model, nb_actions=env.n_actions, memory=memory, nb_steps_warmup=50,\n",
    "                   target_model_update=1e-3, policy=policy, enable_dueling_network=True, \n",
    "                   gamma=.99, batch_size=32, train_interval=100, memory_interval=1)\n",
    "    agent.compile(Adam(lr=1e-6, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=True), metrics=['mae'])\n",
    "    return agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_agent(agent, env, n_steps, debug):\n",
    "    hist = agent.fit(env, nb_steps=n_steps, visualize=debug, log_interval=min(int(n_steps/5),2500), \n",
    "                     verbose=1, history=PokerHistory())\n",
    "    return agent, hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets start with playing against player that always calls or checks based on which is currently valid move\n",
    "# Hopefully this will teach the agent something about hand strength at least\n",
    "env = build_environment(ATM(), False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Next, we build a simple model for DQN\n",
    "def simple_model(window_length):\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=(window_length, env.n_observation_dimensions)))\n",
    "    model.add(Dense(4096, activation='relu'))\n",
    "    #model.add(Dropout(0.4))\n",
    "    model.add(Dense(4096, activation='relu'))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(env.n_actions, activation='softmax'))\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's a more complex model to train later\n",
    "def complex_model(window_length):\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=(window_length, env.n_observation_dimensions)))\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    #model.add(Dropout(0.4))\n",
    "    model.add(Dense(4096, activation='relu'))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(4096, activation='relu'))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(2048, activation='relu'))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(2048, activation='relu'))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(2048, activation='relu'))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(env.n_actions, activation='softmax'))\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A method to iteratively keep playing against previous versions of ourselves\n",
    "def train_loop(training_model, opponent_agent, steps_in_iteration, max_iterations, window_length):\n",
    "    env = build_environment(AIPlayer(opponent_agent, 1), False)\n",
    "    agent = build_agent(training_model(window_length), env.n_actions, window_length, False)\n",
    "    agent, hist = fit_agent(agent, env, steps_in_iteration, False)\n",
    "    visualize_history(hist)\n",
    "    # Save the DQN model\n",
    "    agent.save_weights('weights/loop-0', overwrite=True)\n",
    "    for i in range(max_iterations - 1):\n",
    "        print('ITERATION %s' % str(i + 1))\n",
    "        # Create a copy of the agent to play against us\n",
    "        del opponent_agent # free up resources first\n",
    "        agent.save_weights('weights/temp', overwrite=True)\n",
    "        opponent_agent = build_agent(training_model(window_length), env.n_actions, window_length, False)\n",
    "        opponent_agent.load_weights('weights/temp')\n",
    "        env.swap_opponent_model(opponent_agent)\n",
    "        hist = agent.fit(env, nb_steps=steps_in_iteration, visualize=False, \n",
    "                         log_interval=min(int(steps_in_iteration/5),10000), \n",
    "                         verbose=1, history=PokerHistory())\n",
    "        visualize_history(hist)\n",
    "        dqn.save_weights('weights/loop-%s' % str(i + 1), overwrite=True)\n",
    "    return agent, hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 7856)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4096)              32182272  \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 512)               2097664   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 26)                13338     \n",
      "=================================================================\n",
      "Total params: 51,074,586\n",
      "Trainable params: 51,074,586\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Training for 100000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "2500/2500 [==============================] - 79s 32ms/step - reward: 1.2848\n",
      "877 episodes - episode_reward: 3.662 [-1990.000, 2025.000] - loss: 470284.416 - mean_absolute_error: 19.528 - mean_q: 0.701 - money_won: -4.851\n",
      "\n",
      "Interval 2 (2500 steps performed)\n",
      "2500/2500 [==============================] - 77s 31ms/step - reward: -42.5868\n",
      "877 episodes - episode_reward: -121.399 [-1990.000, 2025.000] - loss: 499582.968 - mean_absolute_error: 20.622 - mean_q: 0.659 - money_won: -48.723\n",
      "\n",
      "Interval 3 (5000 steps performed)\n",
      "2500/2500 [==============================] - 76s 30ms/step - reward: 14.8460\n",
      "880 episodes - episode_reward: 42.176 [-1990.000, 2025.000] - loss: 480489.576 - mean_absolute_error: 20.122 - mean_q: 0.632 - money_won: 8.686\n",
      "\n",
      "Interval 4 (7500 steps performed)\n",
      "2500/2500 [==============================] - 77s 31ms/step - reward: 9.1596\n",
      "877 episodes - episode_reward: 26.111 [-1990.000, 2025.000] - loss: 544823.450 - mean_absolute_error: 22.404 - mean_q: 0.598 - money_won: 3.018\n",
      "\n",
      "Interval 5 (10000 steps performed)\n",
      "2500/2500 [==============================] - 77s 31ms/step - reward: 33.4424\n",
      "868 episodes - episode_reward: 96.320 [-1990.000, 2025.000] - loss: 472562.213 - mean_absolute_error: 19.657 - mean_q: 0.574 - money_won: 27.366\n",
      "\n",
      "Interval 6 (12500 steps performed)\n",
      "2500/2500 [==============================] - 76s 30ms/step - reward: 14.9852\n",
      "861 episodes - episode_reward: 43.511 [-1990.000, 2025.000] - loss: 532825.625 - mean_absolute_error: 21.987 - mean_q: 0.522 - money_won: 8.961\n",
      "\n",
      "Interval 7 (15000 steps performed)\n",
      "2500/2500 [==============================] - 77s 31ms/step - reward: 35.1360\n",
      "883 episodes - episode_reward: 99.479 [-1990.000, 2025.000] - loss: 548030.190 - mean_absolute_error: 22.530 - mean_q: 0.570 - money_won: 28.952\n",
      "\n",
      "Interval 8 (17500 steps performed)\n",
      "2500/2500 [==============================] - 76s 30ms/step - reward: 21.0444\n",
      "870 episodes - episode_reward: 60.472 [-1990.000, 2025.000] - loss: 448387.338 - mean_absolute_error: 18.777 - mean_q: 0.581 - money_won: 14.954\n",
      "\n",
      "Interval 9 (20000 steps performed)\n",
      "2500/2500 [==============================] - 76s 30ms/step - reward: 6.8764\n",
      "860 episodes - episode_reward: 19.990 [-1990.000, 2025.000] - loss: 445829.551 - mean_absolute_error: 18.585 - mean_q: 0.631 - money_won: 0.856\n",
      "\n",
      "Interval 10 (22500 steps performed)\n",
      "2500/2500 [==============================] - 76s 30ms/step - reward: -20.0960 0s - rewar\n",
      "867 episodes - episode_reward: -57.947 [-1990.000, 2025.000] - loss: 481438.636 - mean_absolute_error: 19.633 - mean_q: 0.691 - money_won: -26.162\n",
      "\n",
      "Interval 11 (25000 steps performed)\n",
      "2500/2500 [==============================] - 76s 30ms/step - reward: -31.0112\n",
      "865 episodes - episode_reward: -89.628 [-1990.000, 2025.000] - loss: 496777.203 - mean_absolute_error: 20.443 - mean_q: 0.704 - money_won: -37.069\n",
      "\n",
      "Interval 12 (27500 steps performed)\n",
      "2500/2500 [==============================] - 76s 30ms/step - reward: 18.5296\n",
      "868 episodes - episode_reward: 53.369 [-1990.000, 2025.000] - loss: 475456.022 - mean_absolute_error: 19.653 - mean_q: 0.700 - money_won: 12.454\n",
      "\n",
      "Interval 13 (30000 steps performed)\n",
      "2500/2500 [==============================] - 76s 31ms/step - reward: 0.0204\n",
      "881 episodes - episode_reward: 0.058 [-1990.000, 2025.000] - loss: 554754.547 - mean_absolute_error: 22.683 - mean_q: 0.742 - money_won: -6.144\n",
      "\n",
      "Interval 14 (32500 steps performed)\n",
      "2500/2500 [==============================] - 76s 30ms/step - reward: 11.4992\n",
      "866 episodes - episode_reward: 33.196 [-1990.000, 2025.000] - loss: 494547.256 - mean_absolute_error: 20.533 - mean_q: 0.743 - money_won: 5.437\n",
      "\n",
      "Interval 15 (35000 steps performed)\n",
      "2500/2500 [==============================] - 76s 30ms/step - reward: 24.3072\n",
      "880 episodes - episode_reward: 69.055 [-1990.000, 2025.000] - loss: 505163.787 - mean_absolute_error: 20.826 - mean_q: 0.780 - money_won: 18.147\n",
      "\n",
      "Interval 16 (37500 steps performed)\n",
      "2500/2500 [==============================] - 77s 31ms/step - reward: 1.6076\n",
      "869 episodes - episode_reward: 4.625 [-1990.000, 2025.000] - loss: 507304.244 - mean_absolute_error: 21.097 - mean_q: 0.788 - money_won: -4.478\n",
      "\n",
      "Interval 17 (40000 steps performed)\n",
      "2500/2500 [==============================] - 76s 30ms/step - reward: 17.5060\n",
      "850 episodes - episode_reward: 51.488 [-1990.000, 2025.000] - loss: 493508.417 - mean_absolute_error: 20.412 - mean_q: 0.732 - money_won: 11.556\n",
      "\n",
      "Interval 18 (42500 steps performed)\n",
      "2500/2500 [==============================] - 77s 31ms/step - reward: 2.0672\n",
      "858 episodes - episode_reward: 6.023 [-1990.000, 2025.000] - loss: 455281.313 - mean_absolute_error: 18.836 - mean_q: 0.699 - money_won: -3.939\n",
      "\n",
      "Interval 19 (45000 steps performed)\n",
      "2500/2500 [==============================] - 76s 31ms/step - reward: 28.5296\n",
      "873 episodes - episode_reward: 81.700 [-1990.000, 2025.000] - loss: 497352.209 - mean_absolute_error: 20.465 - mean_q: 0.734 - money_won: 22.422\n",
      "\n",
      "Interval 20 (47500 steps performed)\n",
      "2500/2500 [==============================] - 77s 31ms/step - reward: 21.1264\n",
      "881 episodes - episode_reward: 59.950 [-1990.000, 2025.000] - loss: 473298.657 - mean_absolute_error: 19.403 - mean_q: 0.743 - money_won: 14.956\n",
      "\n",
      "Interval 21 (50000 steps performed)\n",
      "2500/2500 [==============================] - 76s 30ms/step - reward: 11.8972\n",
      "873 episodes - episode_reward: 34.070 [-1990.000, 2025.000] - loss: 496442.148 - mean_absolute_error: 20.384 - mean_q: 0.794 - money_won: 5.789\n",
      "\n",
      "Interval 22 (52500 steps performed)\n",
      "2500/2500 [==============================] - 77s 31ms/step - reward: 31.3304\n",
      "865 episodes - episode_reward: 90.550 [-1990.000, 2025.000] - loss: 469923.560 - mean_absolute_error: 19.370 - mean_q: 0.825 - money_won: 25.272\n",
      "\n",
      "Interval 23 (55000 steps performed)\n",
      "2500/2500 [==============================] - 76s 30ms/step - reward: 4.6676\n",
      "854 episodes - episode_reward: 13.664 [-1990.000, 2025.000] - loss: 457318.629 - mean_absolute_error: 19.057 - mean_q: 0.849 - money_won: -1.310\n",
      "\n",
      "Interval 24 (57500 steps performed)\n",
      "2500/2500 [==============================] - 75s 30ms/step - reward: 12.3696\n",
      "837 episodes - episode_reward: 36.946 [-1990.000, 2025.000] - loss: 468862.449 - mean_absolute_error: 19.578 - mean_q: 0.878 - money_won: 6.514\n",
      "\n",
      "Interval 25 (60000 steps performed)\n",
      "2500/2500 [==============================] - 76s 31ms/step - reward: 24.2924\n",
      "865 episodes - episode_reward: 70.209 [-1990.000, 2025.000] - loss: 498892.511 - mean_absolute_error: 20.522 - mean_q: 0.828 - money_won: 18.234\n",
      "\n",
      "Interval 26 (62500 steps performed)\n",
      "2500/2500 [==============================] - 78s 31ms/step - reward: 37.0452\n",
      "857 episodes - episode_reward: 108.067 [-1990.000, 2025.000] - loss: 458279.668 - mean_absolute_error: 19.269 - mean_q: 0.870 - money_won: 31.049\n",
      "\n",
      "Interval 27 (65000 steps performed)\n",
      "2500/2500 [==============================] - 78s 31ms/step - reward: 5.5964\n",
      "854 episodes - episode_reward: 16.383 [-1990.000, 2025.000] - loss: 517120.374 - mean_absolute_error: 21.333 - mean_q: 0.865 - money_won: -0.382\n",
      "\n",
      "Interval 28 (67500 steps performed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500/2500 [==============================] - 76s 31ms/step - reward: -3.2400\n",
      "854 episodes - episode_reward: -9.485 [-1990.000, 2025.000] - loss: 481735.175 - mean_absolute_error: 20.112 - mean_q: 0.892 - money_won: -9.218\n",
      "\n",
      "Interval 29 (70000 steps performed)\n",
      "2500/2500 [==============================] - 76s 30ms/step - reward: -44.6404\n",
      "887 episodes - episode_reward: -125.818 [-1990.000, 2025.000] - loss: 492234.141 - mean_absolute_error: 20.334 - mean_q: 0.900 - money_won: -50.852\n",
      "\n",
      "Interval 30 (72500 steps performed)\n",
      "  89/2500 [>.............................] - ETA: 1:13 - reward: -167.8652"
     ]
    }
   ],
   "source": [
    "# Let's play for 100000 steps (decisions made by AI)\n",
    "agent = build_agent(simple_model(1), env.n_actions, 1, False)\n",
    "agent, hist = fit_agent(agent, env, FIRST_RUN_STEPS, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some plots of how the training session went\n",
    "visualize_history(hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's evaluate our agent for 5 episodes (hands).\n",
    "agent.test(env, nb_episodes=5, visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets play against our bot with totally random moves now\n",
    "# Hopefully it teaches the agent at least something about how to act on wide range of situations\n",
    "env = build_environment(RandomPlayer(), False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train for 200000 steps\n",
    "agent = build_agent(simple_model(1), env.n_actions, 1, False)\n",
    "agent, hist = fit_agent(agent, env, SECOND_RUN_STEPS, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_history(hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "agent.test(env, nb_episodes=5, visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Window \n",
    "agent, hist = train_loop(complex_model, agent, THIRD_RUN_STEPS, THIRD_RUN_ITERATIONS, THIRD_RUN_WINDOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.test(env, nb_episodes=50, visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's benchmark against ATM\n",
    "env = build_environment(ATM(), False)\n",
    "hist = agent.test(env, nb_episodes=50000, visualize=True)\n",
    "visualize_history(hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's benchmark against RandomPlayer\n",
    "env = build_environment(RandomPlayer(), False)\n",
    "hist = agent.test(env, nb_episodes=50000, visualize=True)\n",
    "visualize_history(hist)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
