{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random search good parameters for models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from players.atm import ATM\n",
    "from players.ai_player import AIPlayer\n",
    "from players.random_player import RandomPlayer\n",
    "from training_env import TrainingEnv\n",
    "from agents import build_dqn_agent, fit_agent, train_loop, load_agent_weights\n",
    "from models import simple_model, complex_model, test_model\n",
    "from util import visualize_history, use_jupyter, release_memory\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_jupyter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many players in table\n",
    "NUMBER_OF_SEATS = 2\n",
    "# Max betsize in simulation environment (shouldn't really matter with discrete relative to pot sizing)\n",
    "MAX_BET = 100000\n",
    "# 'norm' (normalized) or 'one-hot' < how to encode player hand ranking from 7642 unique values\n",
    "RANK_ENCODING = 'norm'\n",
    "\n",
    "WINDOW = 20\n",
    "MODEL = complex_model\n",
    "OPPONENT = RandomPlayer()\n",
    "\n",
    "STEPS = 200000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_params = {\n",
    "    'target_model_update': [0.00001, 0.0001, 0.001, 0.01, 0.1, 0.2, 10, 100, 1000, 10000],\n",
    "    'gamma': [.9, .99, .999, .9999],\n",
    "    'lr': [1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2],\n",
    "    'beta_1': [.9, .99, .999],\n",
    "    'beta_2': [.9, .99, .999, .9999, .99999]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TrainingEnv.build_environment(OPPONENT, NUMBER_OF_SEATS, debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_params(params, env):\n",
    "    \n",
    "    # target_model_update - how often to update target model (or if in 0...1 range, the soft update weight)\n",
    "    # window_length - how many timesteps to look into past (will multiply observation space by this, be careful)\n",
    "    # enable_double_dqn - https://arxiv.org/pdf/1509.06461.pdf\n",
    "    # enable_dueling_network - ???\n",
    "    # train_interval - every how many steps to run a train cycle \n",
    "    # n_warmup_steps - how many steps to run without training\n",
    "    # batch_size - number of (s, a, G) triplets to train on in one training cycle (as a batch)\n",
    "    # gamma - future reward discount essentially\n",
    "    # memory_interval - how often to add last step to memory buffer (discarding every other)\n",
    "    \n",
    "    # lr - learning rate\n",
    "    # beta_1 - L1 normalization\n",
    "    # beta_2 - L2 normalization\n",
    "    # epsilon - Fuzz factor\n",
    "    # decay - Learning rate decay\n",
    "    \n",
    "    #lr_reduction = ReduceLROnPlateau(\n",
    "    #    monitor='episode_reward', factor=0.1, patience=2, verbose=1, mode='auto', min_delta=0.0001, \n",
    "    #    cooldown=0, min_lr=0)\n",
    "    model = MODEL(WINDOW, env.n_observation_dimensions, env.n_actions)\n",
    "    \n",
    "    optimizer = Adam(lr=params['lr'], beta_1=params['beta_1'], beta_2=params['beta_2'], epsilon=None, \n",
    "                     decay=0.0, amsgrad=True)\n",
    "    agent = build_dqn_agent(model, env.n_actions, window_length=WINDOW, \n",
    "                            target_model_update=params['target_model_update'], \n",
    "                            enable_double_dqn=True, enable_dueling_network=True, dueling_type='avg', \n",
    "                            train_interval=100, n_warmup_steps=50, batch_size=32, gamma=params['gamma'], \n",
    "                            memory_interval=1,\n",
    "                            optimizer=optimizer)\n",
    "    agent, hist = fit_agent(agent, env, STEPS, start_from_scratch=True, debug=False)#, callbacks=[lr_reduction])\n",
    "    release_memory(agent)\n",
    "    return sum(hist.history['episode_reward'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params(params=None, chance=.3):\n",
    "    if params is None:\n",
    "        params = {}\n",
    "        chance = 1\n",
    "    for parameter, value_options in all_params.items():\n",
    "        if np.random.random() < chance:\n",
    "            params[parameter] = np.random.choice(value_options)\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = None\n",
    "best_reward = -float('inf')\n",
    "for i in range(100):\n",
    "    new_params = get_params(params=best_params)\n",
    "    reward = test_params(new_params, env)\n",
    "    if reward > best_reward:\n",
    "        best_reward = reward\n",
    "        best_params = new_params\n",
    "        print('New best params:', best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
