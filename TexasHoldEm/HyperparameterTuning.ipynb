{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random search good parameters for models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from players.atm import ATM\n",
    "from players.ai_player import AIPlayer\n",
    "from players.random_player import RandomPlayer\n",
    "from training_env import TrainingEnv\n",
    "from agents import build_dqn_agent, fit_agent, train_loop, load_agent_weights\n",
    "from models import simple_model, complex_model, test_model\n",
    "from util import visualize_history, use_jupyter\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_jupyter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many players in table\n",
    "NUMBER_OF_SEATS = 2\n",
    "# Max betsize in simulation environment (shouldn't really matter with discrete relative to pot sizing)\n",
    "MAX_BET = 100000\n",
    "# 'norm' (normalized) or 'one-hot' < how to encode player hand ranking from 7642 unique values\n",
    "RANK_ENCODING = 'norm'\n",
    "\n",
    "WINDOW = 1\n",
    "MODEL = test_model\n",
    "\n",
    "FIRST_RUN_STEPS = 1000\n",
    "SECOND_RUN_STEPS = 1000\n",
    "THIRD_RUN_STEPS = 1000\n",
    "THIRD_RUN_ITERATIONS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TrainingEnv.build_environment(ATM(), NUMBER_OF_SEATS, debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 396)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                25408     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 26)                442       \n",
      "=================================================================\n",
      "Total params: 28,458\n",
      "Trainable params: 28,458\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = MODEL(WINDOW, env.n_observation_dimensions, env.n_actions)\n",
    "print(model.summary())\n",
    "# window_length - how many timesteps to look into past (will multiply observation space by this, be careful)\n",
    "# enable_double_dqn - https://arxiv.org/pdf/1509.06461.pdf\n",
    "# enable_dueling_network - ???\n",
    "# train_interval - every how many steps to run a train cycle (or if in 0...1 range, the soft update weight)\n",
    "# n_warmup_steps - how many steps to run without training\n",
    "# batch_size - number of (s, a, G) triplets to train on in one training cycle (as a batch)\n",
    "# gamma - future reward discount essentially\n",
    "# memory_interval - how often to add last step to memory buffer (discarding every other)\n",
    "\n",
    "agent = build_dqn_agent(model, env.n_actions, window_length=WINDOW, target_model_update=0.001, \n",
    "                        enable_double_dqn=True, enable_dueling_network=True, dueling_type='avg', \n",
    "                        train_interval=100, n_warmup_steps=50, batch_size=32, gamma=.99, memory_interval=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 1000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "200/200 [==============================] - 7s 34ms/step - reward: 0.3368\n",
      "66 episodes - episode_reward: 1.021 [-55.600, 42.800] - loss: 160.329 - mean_absolute_error: 1.938 - mean_q: 3.178 - money_won: 2.645\n",
      "\n",
      "Interval 2 (200 steps performed)\n",
      "200/200 [==============================] - 4s 21ms/step - reward: 0.6186\n",
      "71 episodes - episode_reward: 1.743 [-58.200, 49.000] - loss: 83.360 - mean_absolute_error: 1.815 - mean_q: 2.952 - money_won: 9.290\n",
      "\n",
      "Interval 3 (400 steps performed)\n",
      "200/200 [==============================] - 4s 20ms/step - reward: 0.2720\n",
      "71 episodes - episode_reward: 0.766 [-63.000, 56.400] - loss: 89.912 - mean_absolute_error: 1.751 - mean_q: 2.715 - money_won: 0.550\n",
      "\n",
      "Interval 4 (600 steps performed)\n",
      "200/200 [==============================] - 4s 20ms/step - reward: 0.4584\n",
      "71 episodes - episode_reward: 1.291 [-66.800, 42.600] - loss: 34.182 - mean_absolute_error: 1.624 - mean_q: 2.819 - money_won: 5.285\n",
      "\n",
      "Interval 5 (800 steps performed)\n",
      "200/200 [==============================] - 4s 20ms/step - reward: 0.0892\n",
      "done, took 22.917 seconds\n"
     ]
    }
   ],
   "source": [
    "# Let's play for 100000 steps (decisions made by AI)\n",
    "agent, hist = fit_agent(agent, env, FIRST_RUN_STEPS, debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
