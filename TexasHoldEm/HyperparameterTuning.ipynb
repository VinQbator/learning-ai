{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random search good parameters for models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from players.atm import ATM\n",
    "from players.ai_player import AIPlayer\n",
    "from players.random_player import RandomPlayer\n",
    "from training_env import TrainingEnv\n",
    "from agents import build_dqn_agent, fit_agent, train_loop, load_agent_weights\n",
    "from models import simple_model, complex_model, test_model\n",
    "from util import visualize_history, use_jupyter, release_memory, print_stats, set_on_demand_memory_allocation\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_on_demand_memory_allocation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_jupyter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many players in table\n",
    "NUMBER_OF_SEATS = 2\n",
    "# Max betsize in simulation environment (shouldn't really matter with discrete relative to pot sizing)\n",
    "MAX_BET = 100000\n",
    "# 'norm' (normalized) or 'one-hot' < how to encode player hand ranking from 7642 unique values\n",
    "RANK_ENCODING = 'norm'\n",
    "\n",
    "WINDOW = 20\n",
    "MODEL = complex_model\n",
    "OPPONENT = RandomPlayer()\n",
    "\n",
    "STEPS = 2000\n",
    "\n",
    "LEARNING_PARAMS_ITERATIONS = 5\n",
    "BATCHING_PARAMS_ITERATIONS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_params = {\n",
    "    'limit': [128, 256, 1024, 4096, 32768],\n",
    "    'target_model_update': [0.00001, 0.0001, 0.001, 0.01, 0.1, 0.2, 10, 100, 1000, 10000],\n",
    "    'batch_sz': [128], #[8, 16, 32, 64],\n",
    "    'train_interval': [64], #[1, 10, 100, 1000],\n",
    "    'gamma': [.9, .99, .999, .9999],\n",
    "    'lr': [1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2],\n",
    "    'beta_1': [.9, .99, .999],\n",
    "    'beta_2': [.9, .99, .999, .9999, .99999]\n",
    "}\n",
    "tried_params = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TrainingEnv.build_environment(OPPONENT, NUMBER_OF_SEATS, debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_params(params, env, steps):\n",
    "    \n",
    "    # target_model_update - how often to update target model (or if in 0...1 range, the soft update weight)\n",
    "    # window_length - how many timesteps to look into past (will multiply observation space by this, be careful)\n",
    "    # enable_double_dqn - https://arxiv.org/pdf/1509.06461.pdf\n",
    "    # enable_dueling_network - ???\n",
    "    # train_interval - every how many steps to run a train cycle \n",
    "    # n_warmup_steps - how many steps to run without training\n",
    "    # batch_size - number of (s, a, G) triplets to train on in one training cycle (as a batch)\n",
    "    # gamma - future reward discount essentially\n",
    "    # memory_interval - how often to add last step to memory buffer (discarding every other)\n",
    "    \n",
    "    # lr - learning rate\n",
    "    # beta_1 - L1 normalization\n",
    "    # beta_2 - L2 normalization\n",
    "    # epsilon - Fuzz factor\n",
    "    # decay - Learning rate decay\n",
    "\n",
    "    warmup_steps = max(params['train_interval'], params['batch_sz']) + 1\n",
    "    \n",
    "    model = MODEL(WINDOW, env.n_observation_dimensions, env.n_actions)\n",
    "    \n",
    "    memory = SequentialMemory(limit=int(params['limit']), window_length=WINDOW)\n",
    "    optimizer = Adam(lr=params['lr'], beta_1=params['beta_1'], beta_2=params['beta_2'], epsilon=None, \n",
    "                     decay=0.0, amsgrad=True)\n",
    "    agent = build_dqn_agent(model, env.n_actions, memory=memory, window_length=WINDOW, \n",
    "                            target_model_update=params['target_model_update'], \n",
    "                            enable_double_dqn=True, enable_dueling_network=True, dueling_type='avg', \n",
    "                            train_interval=params['train_interval'], n_warmup_steps=warmup_steps, \n",
    "                            batch_size=params['batch_sz'], gamma=params['gamma'], memory_interval=1, \n",
    "                            optimizer=optimizer)\n",
    "    agent, hist = fit_agent(agent, env, steps, start_from_scratch=True, verbose=1, debug=False)\n",
    "    print_stats(hist)\n",
    "    release_memory([agent])\n",
    "    return sum(hist.history['episode_reward'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params(params=None, chance=.3):\n",
    "    if params is None:\n",
    "        params = {}\n",
    "        chance = 1\n",
    "    attempts = 0\n",
    "    while True:\n",
    "        for parameter, value_options in all_params.items():\n",
    "            if np.random.random() < chance:\n",
    "                params[parameter] = np.random.choice(value_options)\n",
    "        values = list(params.values())\n",
    "        if not values in tried_params:\n",
    "            tried_params.append(values)\n",
    "            break\n",
    "        attempts += 1\n",
    "        if attempts > 100:\n",
    "            return False     \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = None\n",
    "best_reward = -float('inf')\n",
    "for i in range(LEARNING_PARAMS_ITERATIONS):\n",
    "    print('\\nITERATION %s' % str(i))\n",
    "    new_params = get_params(params=best_params)\n",
    "    if not new_params:\n",
    "        break\n",
    "    reward = test_params(new_params, env, STEPS)\n",
    "    if reward > best_reward:\n",
    "        best_reward = reward\n",
    "        best_params = new_params\n",
    "        print('New best params:', best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_params)\n",
    "# {'limit': 200, 'target_model_update': 1000.0, 'batch_sz': 128, 'train_interval': 50, \n",
    "# 'gamma': 0.9999, 'lr': 1e-05, 'beta_1': 0.999, 'beta_2': 0.9}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_params = {\n",
    "    'limit': [best_params['limit']],\n",
    "    'target_model_update': [best_params['target_model_update']],\n",
    "    'batch_sz': [8, 16, 32, 64, 128],\n",
    "    'train_interval': [1, 10, 100, 1000],\n",
    "    'gamma': [best_params['gamma']],\n",
    "    'lr': [best_params['lr']],\n",
    "    'beta_1': [best_params['beta_1']],\n",
    "    'beta_2': [best_params['beta_2']]\n",
    "}\n",
    "tried_params = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "best_params = None\n",
    "best_reward_rate = -float('inf')\n",
    "for i in range(BATCHING_PARAMS_ITERATIONS):\n",
    "    print('\\nITERATION %s' % str(i))\n",
    "    new_params = get_params(params=best_params)\n",
    "    if not new_params:\n",
    "        break\n",
    "    start_time = time.time()\n",
    "    reward = test_params(new_params, env, STEPS)\n",
    "    time_taken = time.time() - start_time\n",
    "    reward_rate = reward / time_taken\n",
    "    if reward_rate > best_reward_rate:\n",
    "        best_reward_rate = reward_rate\n",
    "        best_params = new_params\n",
    "        print('New best params:', best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
