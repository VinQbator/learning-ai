{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random search good parameters for models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from players.atm import ATM\n",
    "from players.ai_player import AIPlayer\n",
    "from players.random_player import RandomPlayer\n",
    "from training_env import TrainingEnv\n",
    "from agents import build_dqn_agent, fit_agent, train_loop, load_agent_weights\n",
    "from models import simple_model, complex_model\n",
    "from util import visualize_history, use_jupyter\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_jupyter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TrainingEnv.build_environment(ATM(), NUMBER_OF_SEATS, debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MODEL(WINDOW, env.n_observation_dimensions, env.n_actions)\n",
    "print(model.summary())\n",
    "# window_length - how many timesteps to look into past (will multiply observation space by this, be careful)\n",
    "# enable_double_dqn - https://arxiv.org/pdf/1509.06461.pdf\n",
    "# enable_dueling_network - ???\n",
    "# train_interval - every how many steps to run a train cycle (or if in 0...1 range, the soft update weight)\n",
    "# n_warmup_steps - how many steps to run without training\n",
    "# batch_size - number of (s, a, G) triplets to train on in one training cycle (as a batch)\n",
    "# gamma - future reward discount essentially\n",
    "# memory_interval - how often to add last step to memory buffer (discarding every other)\n",
    "\n",
    "agent = build_dqn_agent(model, env.n_actions, window_length=WINDOW, target_model_update=0.001, \n",
    "                        enable_double_dqn=True, enable_dueling_network=True, dueling_type='avg', \n",
    "                        train_interval=100, n_warmup_steps=50, batch_size=32, gamma=.99, memory_interval=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's play for 100000 steps (decisions made by AI)\n",
    "agent, hist = fit_agent(agent, env, FIRST_RUN_STEPS, debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
